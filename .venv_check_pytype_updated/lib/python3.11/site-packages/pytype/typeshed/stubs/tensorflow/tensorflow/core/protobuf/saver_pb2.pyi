"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import sys
import typing

import google.protobuf.descriptor
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message

if sys.version_info >= (3, 10):
    import typing as typing_extensions
else:
    import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor

@typing_extensions.final
class SaverDef(google.protobuf.message.Message):
    """Protocol buffer representing the configuration of a Saver."""

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    class _CheckpointFormatVersion:
        ValueType = typing.NewType("ValueType", builtins.int)
        V: typing_extensions.TypeAlias = ValueType

    class _CheckpointFormatVersionEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[SaverDef._CheckpointFormatVersion.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        LEGACY: SaverDef._CheckpointFormatVersion.ValueType  # 0
        """Internal legacy format."""
        V1: SaverDef._CheckpointFormatVersion.ValueType  # 1
        """Deprecated format: tf.Saver() which works with tensorflow::table::Table."""
        V2: SaverDef._CheckpointFormatVersion.ValueType  # 2
        """Current format: more efficient."""

    class CheckpointFormatVersion(_CheckpointFormatVersion, metaclass=_CheckpointFormatVersionEnumTypeWrapper):
        """A version number that identifies a different on-disk checkpoint format.
        Usually, each subclass of BaseSaverBuilder works with a particular
        version/format.  However, it is possible that the same builder may be
        upgraded to support a newer checkpoint format in the future.
        """

    LEGACY: SaverDef.CheckpointFormatVersion.ValueType  # 0
    """Internal legacy format."""
    V1: SaverDef.CheckpointFormatVersion.ValueType  # 1
    """Deprecated format: tf.Saver() which works with tensorflow::table::Table."""
    V2: SaverDef.CheckpointFormatVersion.ValueType  # 2
    """Current format: more efficient."""

    FILENAME_TENSOR_NAME_FIELD_NUMBER: builtins.int
    SAVE_TENSOR_NAME_FIELD_NUMBER: builtins.int
    RESTORE_OP_NAME_FIELD_NUMBER: builtins.int
    MAX_TO_KEEP_FIELD_NUMBER: builtins.int
    SHARDED_FIELD_NUMBER: builtins.int
    KEEP_CHECKPOINT_EVERY_N_HOURS_FIELD_NUMBER: builtins.int
    VERSION_FIELD_NUMBER: builtins.int
    filename_tensor_name: builtins.str
    """The name of the tensor in which to specify the filename when saving or
    restoring a model checkpoint.
    """
    save_tensor_name: builtins.str
    """The operation to run when saving a model checkpoint."""
    restore_op_name: builtins.str
    """The operation to run when restoring a model checkpoint."""
    max_to_keep: builtins.int
    """Maximum number of checkpoints to keep.  If 0, no checkpoints are deleted."""
    sharded: builtins.bool
    """Shard the save files, one per device that has Variable nodes."""
    keep_checkpoint_every_n_hours: builtins.float
    """How often to keep an additional checkpoint. If not specified, only the last
    "max_to_keep" checkpoints are kept; if specified, in addition to keeping
    the last "max_to_keep" checkpoints, an additional checkpoint will be kept
    for every n hours of training.
    """
    version: global___SaverDef.CheckpointFormatVersion.ValueType
    def __init__(
        self,
        *,
        filename_tensor_name: builtins.str | None = ...,
        save_tensor_name: builtins.str | None = ...,
        restore_op_name: builtins.str | None = ...,
        max_to_keep: builtins.int | None = ...,
        sharded: builtins.bool | None = ...,
        keep_checkpoint_every_n_hours: builtins.float | None = ...,
        version: global___SaverDef.CheckpointFormatVersion.ValueType | None = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["filename_tensor_name", b"filename_tensor_name", "keep_checkpoint_every_n_hours", b"keep_checkpoint_every_n_hours", "max_to_keep", b"max_to_keep", "restore_op_name", b"restore_op_name", "save_tensor_name", b"save_tensor_name", "sharded", b"sharded", "version", b"version"]) -> None: ...

global___SaverDef = SaverDef
